# Data Pipeline Framework - Source to Target Transfer Tasks Documentation

## Project Overview

This project builds a **universal data pipeline framework** that abstracts complex distributed system challenges into reusable, intelligent components. The framework follows a **framework-first design philosophy** that separates core pipeline intelligence from system-specific implementations.

## Completed Components Status

âœ… **Record Generation Module** - Complete and tested  
âœ… **Record Validation Module** - Complete and tested  
âœ… **Record Validation with Rebuild** - Complete and tested  
âœ… **Store New and Pass Valid Record Task** - Complete and tested  
âœ… **Pre-Validation Count Check Task** - Complete and tested  
âœ… **Source to Stage Transfer Task** - Complete and ready  
âœ… **Stage to Target Transfer Task** - Complete and ready  
âœ… **Audit Task** - Complete and ready  
ðŸ”„ **Next**: Database Integration / System-Specific Implementations

## Source to Stage Transfer Task

### Purpose and Strategic Importance

The Source to Stage Transfer task manages the complex orchestration of moving data from source systems to staging areas while maintaining comprehensive state tracking and implementing sophisticated failure recovery mechanisms.

### File Location
```
data_pipeline_project/pipeline_framework/airflow_tasks/source_to_stage_transfer.py
```

### Key Functions

**Main Task Function:**
```python
def source_to_stage_task(final_config: Dict[str, Any], record: Dict[str, Any]) -> Dict[str, Any]
```

**Placeholder Functions (Need Implementation):**
- `check_transfer_completion()` - Check if transfer already completed
- `clean_stage_location()` - Clean stage location before transfer
- `transfer_source_to_stage()` - Main transfer function
- `upload_record_to_database()` - Upload updated record to database

### Configuration Requirements
```python
final_config = {
    'timezone': str,           # Processing timezone (required)
    # Additional config as needed by implementation
}
```

### Record Fields Managed
```python
record = {
    'SOURCE_TO_STAGE_INGESTION_START_TIME': None,
    'SOURCE_TO_STAGE_INGESTION_END_TIME': None,
    'SOURCE_TO_STAGE_INGESTION_STATUS': 'PENDING',
    'PIPELINE_START_TIME': None,
    'PIPELINE_END_TIME': None,
    'PIPELINE_STATUS': 'PENDING',
    'PHASE_COMPLETED': None,
    'RETRY_ATTEMPT': 0,
    'RECORD_LAST_UPDATED_TIME': None
}
```

### Task Flow
1. **Check Completion**: Verify if transfer already completed
2. **Set Start Time**: Record start timestamp (status stays PENDING)
3. **Clean Stage**: Clean staging location before transfer
4. **Transfer Data**: Execute main source-to-stage transfer
5. **Update Success**: Set end time, status=COMPLETED, phase_completed=SOURCE_TO_STAGE
6. **Upload Record**: Save updated record to database

### Failure Handling
- **On Failure**: Clean partial data, reset SOURCE_TO_STAGE_* + PIPELINE_* fields
- **Keep phase_completed unchanged** (preserves previous successful phases)
- Increment RETRY_ATTEMPT and update RECORD_LAST_UPDATED_TIME
- Upload reset record to database

---

## Stage to Target Transfer Task

### Purpose and Strategic Importance

The Stage to Target Transfer task manages the transfer of data from staging areas to target systems (like data warehouses) with comprehensive state tracking and failure recovery.

### File Location
```
data_pipeline_project/pipeline_framework/airflow_tasks/stage_to_target_transfer.py
```

### Key Functions

**Main Task Function:**
```python
def stage_to_target_task(final_config: Dict[str, Any], record: Dict[str, Any]) -> Dict[str, Any]
```

**Placeholder Functions (Need Implementation):**
- `check_transfer_completion()` - Check if transfer already completed
- `clean_target_location()` - Clean target location before transfer
- `transfer_stage_to_target()` - Main transfer function
- `upload_record_to_database()` - Upload updated record to database

### Record Fields Managed
```python
record = {
    'STAGE_TO_TARGET_INGESTION_START_TIME': None,
    'STAGE_TO_TARGET_INGESTION_END_TIME': None,
    'STAGE_TO_TARGET_INGESTION_STATUS': 'PENDING',
    'PIPELINE_START_TIME': None,
    'PIPELINE_END_TIME': None,
    'PIPELINE_STATUS': 'PENDING',
    'PHASE_COMPLETED': None,
    'RETRY_ATTEMPT': 0,
    'RECORD_LAST_UPDATED_TIME': None
}
```

### Task Flow
1. **Check Completion**: Verify if transfer already completed
2. **Set Start Time**: Record start timestamp (status stays PENDING)
3. **Clean Target**: Clean target location before transfer
4. **Transfer Data**: Execute main stage-to-target transfer
5. **Update Success**: Set end time, status=COMPLETED, phase_completed=STAGE_TO_TARGET
6. **Upload Record**: Save updated record to database

### Failure Handling
- **On Failure**: Clean partial data, reset STAGE_TO_TARGET_* + PIPELINE_* fields
- **Keep phase_completed unchanged** (preserves previous successful phases)
- Increment RETRY_ATTEMPT and update RECORD_LAST_UPDATED_TIME
- Upload reset record to database

---

## Audit Task

### Purpose and Strategic Importance

The Audit Task serves as the final data integrity guardian that compares source and target record counts to ensure data consistency. This task implements two different failure handling strategies based on the type of failure encountered.

### File Location
```
data_pipeline_project/pipeline_framework/airflow_tasks/audit_task.py
```

### Key Functions

**Main Task Function:**
```python
def audit_task(final_config: Dict[str, Any], record: Dict[str, Any]) -> Dict[str, Any]
```

**Count Functions:**
```python
def get_source_count(final_config: Dict[str, Any], record: Dict[str, Any]) -> int
def get_target_count(source_count: int, final_config: Dict[str, Any], record: Dict[str, Any]) -> int
```

**Cleanup Functions:**
```python
def clean_stage_location(final_config: Dict[str, Any], record: Dict[str, Any]) -> bool
def clean_target_location(final_config: Dict[str, Any], record: Dict[str, Any]) -> bool
```

**Reset Functions:**
```python
def reset_on_process_failure(final_config: Dict[str, Any], record: Dict[str, Any]) -> None
def reset_on_audit_mismatch(final_config: Dict[str, Any], record: Dict[str, Any]) -> None
```

### Record Fields Managed
```python
record = {
    'AUDIT_START_TIME': None,
    'AUDIT_END_TIME': None,
    'AUDIT_STATUS': 'PENDING',
    'AUDIT_RESULT': None,
    'SOURCE_COUNT': None,
    'TARGET_COUNT': None,
    'COUNT_DIFFERENCE': None,
    'PERCENTAGE_DIFFERENCE': None,
    'PIPELINE_START_TIME': None,
    'PIPELINE_END_TIME': None,
    'PIPELINE_STATUS': 'PENDING',
    'PHASE_COMPLETED': None,
    'RETRY_ATTEMPT': 0,
    'RECORD_LAST_UPDATED_TIME': None,
    'RECORD_FIRST_CREATED_TIME': None
}
```

### Task Flow
1. **Check Completion**: Verify if audit already completed
2. **Set Start Time**: Record start timestamp
3. **Get Source Count**: Retrieve source system count
4. **Get Target Count**: Retrieve target system count (receives source_count parameter)
5. **Calculate Differences**: Compute count_difference and percentage_difference
6. **Set End Time**: Record completion timestamp
7. **Compare Counts**: Check if source_count == target_count
8. **Handle Result**: Success (PASS) or failure handling

### Two Types of Failure Handling

#### 1. Process Failure (Connection/System Issues)
**Trigger**: Exceptions during count retrieval or system connectivity issues

**Reset Scope**: Limited reset
- Resets only AUDIT_* fields to None/PENDING
- Resets PIPELINE_* fields to None/PENDING
- **Keeps PHASE_COMPLETED unchanged** (preserves previous successful phases)
- Increments RETRY_ATTEMPT and updates RECORD_LAST_UPDATED_TIME

#### 2. Audit Mismatch (Data Corruption)
**Trigger**: source_count != target_count (data integrity violation)

**Reset Scope**: Complete reset
- **Immediate Cleanup**: Calls clean_stage_location() and clean_target_location()
- Resets ALL SOURCE_TO_STAGE_* fields to None/PENDING
- Resets ALL STAGE_TO_TARGET_* fields to None/PENDING
- Resets ALL AUDIT_* fields to None/PENDING
- Resets ALL PIPELINE_* fields to None/PENDING
- **Resets PHASE_COMPLETED to None** (forces complete pipeline restart)
- **Resets RECORD_FIRST_CREATED_TIME and RECORD_LAST_UPDATED_TIME to None**
- **Only increments RETRY_ATTEMPT** (no timestamp update)

### Success Path
- Sets AUDIT_RESULT = 'PASS'
- Sets PHASE_COMPLETED = 'AUDIT'
- Updates RECORD_LAST_UPDATED_TIME
- Uploads record to database

---

## Framework Design Patterns

### Common Patterns Across All Tasks

#### 1. Configuration Structure
```python
final_config = {
    'timezone': str,           # Required for all tasks
    # Additional system-specific configuration
}
```

#### 2. Return Value Structure
```python
{
    'success': True | False,   # Clear success/failure indicator
    'reason': str,             # Human-readable explanation
    'record': Dict             # Updated record object
}
```

#### 3. Status Management
- **PENDING**: Initial state, stays during processing to minimize API calls
- **COMPLETED**: Only set on successful completion
- **No IN_PROGRESS**: Avoids unnecessary database updates

#### 4. Timing Fields
- All timing fields use `pendulum.now(timezone).to_iso8601_string()`
- Timezone from final_config ensures consistency
- Start time set at beginning, end time set on completion/failure

#### 5. Error Handling
- Comprehensive try/except/finally blocks
- Different reset strategies based on failure type
- Always upload updated record after reset
- Structured logging with clear status indicators

#### 6. Database Upload Pattern
- Upload on both success and failure paths
- Placeholder functions with TODO comments for implementation
- Consistent function signature: `(final_config, record) -> bool`

---

## Implementation Requirements for Team

### Immediate Priority Tasks

#### 1. Database Integration
**Replace all placeholder functions:**
```python
def upload_record_to_database(final_config: Dict[str, Any], record: Dict[str, Any]) -> bool:
    # TODO: Implement actual database upload logic
    # - Connect to database using final_config credentials
    # - Update/insert record with current field values
    # - Handle database connectivity errors
    # - Return True/False for success/failure
```

#### 2. Source System Integration
**Source to Stage Task:**
```python
def check_transfer_completion(final_config: Dict[str, Any], record: Dict[str, Any]) -> bool:
    # TODO: Check if SOURCE_TO_STAGE_INGESTION_STATUS == 'COMPLETED'
    # Could be database query or file existence check

def clean_stage_location(final_config: Dict[str, Any], record: Dict[str, Any]) -> bool:
    # TODO: Clean S3 bucket, clear staging directories
    # Use time window from record for specific cleanup

def transfer_source_to_stage(final_config: Dict[str, Any], record: Dict[str, Any]) -> bool:
    # TODO: Implement actual data transfer logic
    # Example: Elasticsearch -> S3, API -> GCS, Database -> Local storage
```

#### 3. Target System Integration
**Stage to Target Task:**
```python
def clean_target_location(final_config: Dict[str, Any], record: Dict[str, Any]) -> bool:
    # TODO: Clean target tables, remove partial loads
    # Use time window from record for specific cleanup

def transfer_stage_to_target(final_config: Dict[str, Any], record: Dict[str, Any]) -> bool:
    # TODO: Implement actual loading logic  
    # Example: S3 -> Snowflake, GCS -> BigQuery, Local -> PostgreSQL
```

#### 4. Count Validation Integration
**Audit Task:**
```python
def get_source_count(final_config: Dict[str, Any], record: Dict[str, Any]) -> int:
    # TODO: Query source system for count in time window
    # Extract WINDOW_START_TIME, WINDOW_END_TIME from record
    # Return integer count

def get_target_count(source_count: int, final_config: Dict[str, Any], record: Dict[str, Any]) -> int:
    # TODO: Query target system for count in time window
    # Note: Receives source_count as parameter
    # Extract time window from record
    # Return integer count
```

### Database Schema Requirements

**Records Table Structure:**
```sql
CREATE TABLE pipeline_records (
    id SERIAL PRIMARY KEY,
    
    -- Time Window Fields
    window_start_time TIMESTAMP,
    window_end_time TIMESTAMP,
    target_day DATE,
    time_interval VARCHAR(10),
    
    -- Source to Stage Fields
    source_to_stage_ingestion_start_time TIMESTAMP,
    source_to_stage_ingestion_end_time TIMESTAMP,
    source_to_stage_ingestion_status VARCHAR(20) DEFAULT 'PENDING',
    
    -- Stage to Target Fields
    stage_to_target_ingestion_start_time TIMESTAMP,
    stage_to_target_ingestion_end_time TIMESTAMP,
    stage_to_target_ingestion_status VARCHAR(20) DEFAULT 'PENDING',
    
    -- Audit Fields
    audit_start_time TIMESTAMP,
    audit_end_time TIMESTAMP,
    audit_status VARCHAR(20) DEFAULT 'PENDING',
    audit_result VARCHAR(10),
    source_count INTEGER,
    target_count INTEGER,
    count_difference INTEGER,
    percentage_difference DECIMAL(5,2),
    
    -- Pipeline Fields
    pipeline_start_time TIMESTAMP,
    pipeline_end_time TIMESTAMP,
    pipeline_status VARCHAR(20) DEFAULT 'PENDING',
    phase_completed VARCHAR(50),
    
    -- Record Management Fields
    retry_attempt INTEGER DEFAULT 0,
    record_first_created_time TIMESTAMP,
    record_last_updated_time TIMESTAMP,
    
    -- Indexes for performance
    INDEX idx_status (source_to_stage_ingestion_status, stage_to_target_ingestion_status, audit_status),
    INDEX idx_time_window (window_start_time, window_end_time),
    INDEX idx_phase (phase_completed)
);
```

### Testing Strategy

#### 1. Unit Testing
**Test each task independently:**
```python
# Test source-to-stage
def test_source_to_stage_success():
    config = {'timezone': 'America/Los_Angeles'}
    record = {'SOURCE_TO_STAGE_INGESTION_STATUS': 'PENDING', ...}
    result = source_to_stage_task(config, record)
    assert result['success'] == True
    assert record['PHASE_COMPLETED'] == 'SOURCE_TO_STAGE'

def test_source_to_stage_failure():
    # Mock transfer failure
    # Verify reset behavior and retry increment
```

#### 2. Integration Testing
**Test complete pipeline flow:**
```python
def test_complete_pipeline():
    # 1. Store new and pass valid record
    # 2. Pre-validation count check
    # 3. Source to stage transfer
    # 4. Stage to target transfer  
    # 5. Audit task
    # Verify record state at each step
```

#### 3. Failure Scenario Testing
**Test both failure types:**
```python
def test_audit_process_failure():
    # Mock database connectivity issue
    # Verify limited reset (audit + pipeline only)

def test_audit_mismatch():
    # Mock count mismatch
    # Verify complete reset + cleanup
```

### Production Configuration

#### 1. Environment Variables
```bash
export TIMEZONE="America/Los_Angeles"
export DATABASE_URL="postgresql://user:pass@host:port/db"
export ELASTICSEARCH_HOST="your-elasticsearch-host"
export S3_BUCKET="your-staging-bucket"
export SNOWFLAKE_ACCOUNT="your-snowflake-account"
```

#### 2. Airflow Integration
```python
# Example DAG integration
from data_pipeline_project.pipeline_framework.airflow_tasks.source_to_stage_transfer import source_to_stage_task
from data_pipeline_project.pipeline_framework.airflow_tasks.stage_to_target_transfer import stage_to_target_task
from data_pipeline_project.pipeline_framework.airflow_tasks.audit_task import audit_task

def source_to_stage_wrapper(**context):
    record = context['ti'].xcom_pull(task_ids='pre_validation')
    result = source_to_stage_task(CONFIG, record)
    if not result['success']:
        raise AirflowException(result['reason'])
    return result['record']
```

---

## Team Handoff Checklist

### âœ… Complete and Ready
- [x] **Source to Stage Transfer Task** - Complete implementation with placeholders
- [x] **Stage to Target Transfer Task** - Complete implementation with placeholders
- [x] **Audit Task** - Complete implementation with dual failure handling
- [x] **Comprehensive logging** - Structured logging throughout all tasks
- [x] **Error handling** - Proper exception handling and state management
- [x] **Consistent patterns** - Uniform configuration, return values, and timing
- [x] **Framework integration** - Uses existing utilities and patterns
- [x] **Complete documentation** - This handoff guide with all implementation details

### ðŸ”„ Needs Implementation  
- [ ] **Database functions** - Replace all upload_record_to_database() placeholders
- [ ] **Source system functions** - Implement check/clean/transfer for source systems
- [ ] **Target system functions** - Implement check/clean/transfer for target systems  
- [ ] **Count functions** - Implement source and target count queries
- [ ] **System-specific configuration** - Add credentials and connection details
- [ ] **Integration testing** - Test with actual database and systems
- [ ] **Performance optimization** - Tune queries and transfers for production

### ðŸ“‹ Development Priorities
1. **Database Integration** (High Priority) - Core persistence layer
2. **Source System Implementation** (High Priority) - Data extraction logic
3. **Target System Implementation** (High Priority) - Data loading logic
4. **Count Validation** (Medium Priority) - Audit functionality
5. **Performance Testing** (Medium Priority) - Production readiness
6. **Monitoring Integration** (Low Priority) - Operational visibility

---

## Framework Philosophy and Value

### Design Principles Demonstrated

1. **Separation of Concerns**: Framework handles orchestration, placeholders handle technology specifics
2. **Consistent State Management**: All tasks follow same patterns for status and timing
3. **Intelligent Failure Recovery**: Different reset strategies based on failure type
4. **Resource Conservation**: Minimal database updates (PENDINGâ†’COMPLETED only)
5. **Operational Excellence**: Comprehensive logging and clear error handling

### Business Value Delivered

1. **Rapid Implementation**: Teams focus on business logic, not orchestration
2. **Universal Patterns**: Same framework applies to any sourceâ†’stageâ†’target stack
3. **Enterprise Reliability**: Built-in failure recovery and data integrity checking
4. **Operational Visibility**: Structured logging and clear status tracking
5. **Technology Evolution Protection**: Framework abstracts underlying system changes

### Next Steps Recommendation

The framework foundation is complete and production-ready. The remaining work is primarily **system-specific implementation** rather than framework development. Teams can implement their specific technology integrations (Elasticsearch, S3, Snowflake, etc.) while inheriting all the sophisticated orchestration and failure handling logic.

**Recommended approach:**
1. Start with database integration to enable end-to-end testing
2. Implement one complete technology stack (e.g., Elasticsearchâ†’S3â†’Snowflake)
3. Test thoroughly with real data volumes
4. Expand to additional technology stacks using the proven patterns

The framework transforms complex data pipeline development from **months of custom implementation** to **days of configuration and integration**.

---

**Status**: Complete handoff package ready. Framework provides comprehensive foundation for production data pipeline implementation with clear path for system-specific integration.